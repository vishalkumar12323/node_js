Now let's talk about the different types of streams that we have available.

Now mainly we have two streams.

We have a reliable stream and a readable stream.

Now, a readable stream would be a stream that you could write to.

Like that write million times example that we wrote, we were writing to a file, so we used a readable

stream.

Now a readable stream is a stream that you would read from.

So you could imagine that you might want to, for example, open a file that's very huge, like ten

gigabytes and you want to read from it.

And then using another write stream, you want to write that to another file.

So that's where you would use a readable stream.

Now in our previous if I go back to the previous slide here, we could see that this is a readable stream

because we're writing to ffmpeg.

Now there is another one from ffmpeg to node, which I haven't actually drawn here, but you could imagine

another stream from ffmpeg to node.

But that is a readable stream because notice is reading from that.

Now this one from node to network card.

Again we could have both a readable stream and a readable stream.

If you're reading from the requests, we're using a readable stream.

If we're writing to the responses, we're using a readable stream.

So if we send a request back to the client we are writing, and if we are reading the requests coming

from the client we are reading.

Now in terms of our storage, because we're just saving those files on our hard drive, it will be just

one writable stream or multiple write streams because we could be handling many requests at the same

time.

So we would be writing to different files simultaneously.

But yeah, these two are the fundamental types of streams in node.

We also have two more streams and one of them is duplex, which is just a stream that's both writable

and readable.

We also have a transform stream, which is just like duplex, but it also transforms the data.

Now we're going to get into these two later.

If you understand the readable stream and the readable stream, understanding these two will be really

easy.

So whenever we have a flow of data.

Streams are an incredible option to use.

So this flow of data could be between nodes and another process or node and an underlying resource like

hardware.

Or it could just be really any kind of data flowing.

And by data we just mean zeros and ones.

All right.

So for example let's say that you have this piece of data and you want to encrypt it.

So you want to do some operations and make this data encrypted so that nobody could access it unless

they have access to secret codes.

Now what you could do is to create a map and say, whenever I have A00 and A11, I'm just going to change

it to A111 and A000.

So we're going to look for in the data and see where we have these patterns.

And just to note, these encryptions and compressions and things like these are actually working with

the raw binary data with these zeros and ones in most cases.

So what you would do is to just find those keys and then just do a replace and end up with something

that's really not meaningful to somebody that doesn't understand this map.

So this part will be something gibberish.

If someone takes this data and runs it through a character encoding, they're going to end up with some

gibberish because we have done replace here.

Now, if you give this to the right user and now they want to decrypt it, what they could do is to

just use that map and do another replace and go backwards and then get the original data.

Now in real world encryption applications, it's actually more complex than this.

There are a lot of prime numbers involved, a lot of discrete math.

So it's not just doing a replace on one portion of the code and then leaving the rest as they are.

It's just more like doing an encryption on all of the data using some mathematical formulas.

So it's more complex and also way harder to crack.

But this is also a valid example of an encryption.

You just have to make sure to keep an eye out on those keys so that you know that this is an actual

key and you have to do to replace, and it's not part of the data.

But anyway, now this is an example of data flowing.

Imagine you want to encrypt a huge message that's hundreds of megabytes, or maybe a couple of gigabytes

or even more.

It's not really memory efficient to just move all that data into your memory and then do your encryption.

What you would do instead is to just try to get different chunks out of that original message, be it

a file or a response.

It doesn't matter.

Just be any kind of data.

It doesn't even have to be a string, just some raw binary data.

So you want to get different chunks out of this file and then do these operations and make it encrypted,

and then write it to a destination.

And this is a good example of using a transform stream.

So we just read the data, transform it and write it to a destination.

Another example would be if you want to compress a data.

Now in this case you would kind of go the other way.

So you keep looking for patterns, you keep looking for repetitions, and you try to condense them down

to something smaller.

For example, you want to have a map like this and say, I have this many ones and double zero and then

this many ones.

I'm just going to condense it to zero one, zero one, zero one.

And then you're going to look for those patterns, which we have three here in this binary data.

And then you're going to do your replace.

So what's going to happen is that you're going to end up with a file that's much smaller in size compared

to your original file.

And then using this map you could just get the original file.

Now you might compress some data, and the file size might not really change because there might not

be that much repetition or that many patterns to condense them down to something else.

But some cases, especially if there is a ton of manual repetition, you could really reduce the file

size by so much.

But again, it really depends on the data that you're working with.

But here could be another example of using a transform stream.

The point is, whenever you have this data flowing, you could use streams.

You could use the readable stream to just read.

You could use the readable stream to just write duplex if you want to do both and transform if you want

to modify the data in any way.

Right.

So now let's understand what happens inside of a stream object.

Now we know what streams are.

Let's now see how they work.

More specifically, right now let's talk about a writable stream.

So let's say that this is our writable stream object.

And we can get this by running something like FS dot create right string.

Something that we did in our previous example.

We wrote that code and then we got a stream object.

And this is our object.

This is our writable stream.

Now inside of this writable stream object we have an internal buffer.

And its size is 16,384 bytes by default, which is specified by the high water mark value.

And we're going to see that in action later in the code.

But this is the default size and you can change it.

But this is by default how much of a buffer will be allocated to each stream a note.

Now apart from this internal buffer, each stream object, each writable stream object also has some

events, properties, and methods available on it.

Now one of the most important methods is this stream right methods.

Now, whenever you call this function this method, what's going to happen is that you're going to push

some amount of data into your internal buffer.

Let's say you try to write a buffer that's of size 300 bytes.

Now this data here is should be a buffer.

Now you can also specify a string.

But notice later on we'll use the character encoding specified to convert this data this string to a

buffer.

And then do the right.

So this data think of it just like a buffer.

Although you could specify a string.

But think of it like that.

You could only specify a buffer just to make it easy for you to understand how this works.

So you push some amount of data into this internal buffer and you keep pushing.

So you push another chunk chunk another one, another one, another one, and you keep going until your

buffer is completely filled.

So now that we have our buffer completely filled, what the string will do in this case is that it will

do a write.

So it will now pull all of this data out in one chunk and do one write.

This is how a writable stream works in node.

Now the reason that it's so efficient, one of the reasons actually is if you take a look we wrote eight

times.

So if you count these there are eight different chunks.

So we wrote to that stream object eight times.

Now in our example we have that loop.

And we kept writing these numbers to our file.

So we wrote one actually space a number and then a space.

So let's say if that string is like five characters so five characters, each character is eight bits.

So five times eight is 40 bits way smaller than this 16kB.

That is the size of this buffer.

So we kept pushing those small data to that stream.

So in this example we wrote eight times.

But the important point here is that we only write once.

So we write to our stream eight times but we only write once.

Now when you write your writing to an underlying resource like a file for example, or your network

card, now this is way more efficient because you're writing now and your memory and your process and

your NodeJS process.

And then once the data is filled, you're just going to do a right now in our example of that for loop

and writing those numbers.

Now without streams we're writing 1 million times to our underlying resource.

But if we use streams, what will happen is that we will keep gathering the data and then do one write.

So we will significantly decrease the number of writes that we do into that file, which is way more

efficient.

Now you might be asking what will happen if we try to write to the stream an amount of data that's way

bigger than the internal buffer, and I just way bigger, just slightly bigger.

Let's say that here, our last write was 2000 and 177 bytes.

Now, what will happen if we try to write a data that's maybe like 3000 bytes for our last write to

completely fill this buffer?

Now what will happen is that node will push this amount of bytes to our internal buffer and then buffer

the rest of the data, which is going to be around 800 bytes.

Now that 800 bytes will be in memory, it will keep track of it.

And once the internal buffer is emptied, it's going to push that again to this internal buffer.

Now, if you think about it, this case, we're going to have a problem.

What if you keep writing to this internal buffer without letting it to get emptied?

Now, what's going to happen is that Node.js will keep buffering all of this data.

And by buffering, I just want to say this one more time.

A buffer is a location in memory that holds a specific amount of data.

So if you create a buffer that's 400MB in size, you are quickly filling your memory by 400MB.

So why don't we have a buffer?

Whatever its size is, you have allocated that much size off of your memory.

All right.

So if you keep buffering data it's going to be a problem because you're going to run into some memory

issues.

For example, if you try to write a data that's 800MB in size, what will happen is that, well, first

of all, the buffer will get filled.

And we're going to have this 16kB in our internal buffer.

But notice now we'll keep the rest of the data in memory.

Right.

Which is going to be a huge problem.

And in this case our memory usage will jump up to 800MB, which is not something that we want.

We're using streams to try to lower this amount as much as we possibly can.

But if we try to do this, if we try to run this code, if we try to say stream dot writes and put a

data that's very huge, we're going to have some memory issues or not just doing one writes, but you

keep writing without letting the buffer get emptied.

Like our previous example, we kept writing to that stream, but we didn't really let it to get empty.

Right?

We just kept writing, and now we just kept buffering all those incoming data so that we had that huge

amount of memory issue.

I guess it was like 700MB, I don't recall, but it was a huge amount, which is something that we don't

want.

So what you need to do is to first wait for this internal buffer to get emptied and then do another

right now, this time, this process that the stream does to empty this buffer is called draining.

And we have to keep an eye out on this event.

And it's actually an event.

So you say a writeable stream dot on drain and then you do your code.

We're going to see all that in code.

But I just want to say that you have to wait for this internal buffer to get emptied.

And after it's completely empty, you are now safe to push more data into this internal buffer.

Or I should say, right.

So this is how a readable stream works in node.

It's just an object that has some events, properties and methods, and also this internal buffer.

Let's now take a look at the readable stream object.

Let's see how this works.

So again we have our stream object.

And we can get this by running something like Fstat create read stream.

And what we're going to end up with is this object.

Now just like a readable stream, we have an internal buffer size of which is that high watermark value,

which is by default 16,384 bytes or just 16kB.

Now again, just like a right old stream.

We have some events, properties and methods available on this readable stream.

The way that we can push data into this readable stream is by calling stream, dot, push, and then

some amount of data.

So we keep pushing into this internal buffer.

And what's going to happen is that once it's filled we're going to get an event.

And the event is called Stream on data.

So the event name is data.

So you need to run this code on your readable stream and keep looking for these data events.

So once the internal buffer is full we're going to get an event called data.

And we're going to get this data all in one chunk here in this callback function.

So we're going to pull the data out and we're going to do whatever we want with it.

And this three dots section maybe you want to write it to another readable stream or you just want to

read it.

It's up to you.

So this is a readable stream, and you use this readable stream to make a huge data, a huge message

into different chunks.

So let's go back into our previous example.

Here let's say that you have this 800MB of data.

Let's say it's a message coming to your network card.

Maybe it's a request or if it's a file you want to read.

So what should you do in this case?

What do you think?

Let's say that I want to write a data to a readable stream to a writeable.

Excuse me.

Maybe it's a file and the data is so huge, what should I do in that case?

Well, what you should do is to first create a readable stream off of this data, be it a file or a

request, anything.

You first create a readable stream, and then when you create a readable stream, you're going to get

the data in chunks of this size, this 16kB, 16kB of size.

So you get the data and then you write to this readable stream.

So this is what you should do.

If you have a huge amount of data and you want to write it somewhere.

First create a readable stream and then you write it.

Right.

So this is really how this readable stream and readable stream work.

Now what about duplex and transform.

Now they're just like these two.

A duplex is just a stream that you could both write to and read from.

So a writeable stream and a readable stream.

They only have one internal buffer, only one, but a duplex has two.

A transform has two.

One for reading and one for writing.

So these are all the streams we have in a nutshell.

Now let's jump right into our code editor and try to use these streams, and let's see some interesting

results.

Let's first fix our problem with that memory issue.

